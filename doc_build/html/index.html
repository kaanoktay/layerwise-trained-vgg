
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Welcome to scaling-spoon’s documentation! &#8212; scaling-spoon 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="welcome-to-scaling-spoon-s-documentation">
<h1>Welcome to scaling-spoon’s documentation!<a class="headerlink" href="#welcome-to-scaling-spoon-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
<span class="target" id="module-loaders.cifar10"></span><div class="section" id="cifar10-py">
<h2>cifar10.py<a class="headerlink" href="#cifar10-py" title="Permalink to this headline">¶</a></h2>
<p>Dataset wrappers to use cifar10 in a semi-supervised fashion.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#loaders.cifar10.CifarSubsetSampler" title="loaders.cifar10.CifarSubsetSampler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">loaders.cifar10.CifarSubsetSampler</span></code></a>(indices)</p></td>
<td><p>This class randomly permutes a set of indicies of the cifar10 dataset.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#loaders.cifar10.semi_supervised_cifar10" title="loaders.cifar10.semi_supervised_cifar10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">loaders.cifar10.semi_supervised_cifar10</span></code></a>(…)</p></td>
<td><p>Provides the three dataloaders:</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="loaders.cifar10.CifarSubsetSampler">
<em class="property">class </em><code class="sig-prename descclassname">loaders.cifar10.</code><code class="sig-name descname">CifarSubsetSampler</code><span class="sig-paren">(</span><em class="sig-param">indices</em><span class="sig-paren">)</span><a class="headerlink" href="#loaders.cifar10.CifarSubsetSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.sampler.Sampler</span></code></p>
<p>This class randomly permutes a set of indicies of the cifar10 dataset.
We need this sampler to restrict the dataset class to only provide
permuted batches of data from one one the strictly separated supervised
and unsupervised sets.</p>
</dd></dl>

<dl class="function">
<dt id="loaders.cifar10.semi_supervised_cifar10">
<code class="sig-prename descclassname">loaders.cifar10.</code><code class="sig-name descname">semi_supervised_cifar10</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">transformation_id: str</em>, <em class="sig-param">supervised_ratio=0.1</em>, <em class="sig-param">batch_size=128</em>, <em class="sig-param">download=False</em>, <em class="sig-param">augmentation=False</em>, <em class="sig-param">num_workers=(6</em>, <em class="sig-param">6</em>, <em class="sig-param">2)</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]<a class="headerlink" href="#loaders.cifar10.semi_supervised_cifar10" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides the three dataloaders:</p>
<ul class="simple">
<li><p>Testset Dataloader</p></li>
<li><p>Training Set Dataloader (Supervised Dataset)</p></li>
<li><p>Training Set Dataloader (Unsupervised Dataset)</p></li>
</ul>
<p>It wraps the pytorch cifar10 dataset class.</p>
</dd></dl>

</div>
<span class="target" id="module-loaders.configloader"></span><div class="section" id="configloader-py">
<h2>configloader.py<a class="headerlink" href="#configloader-py" title="Permalink to this headline">¶</a></h2>
<p>A yaml config loader.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">loaders.ConfigLoader</span></code></p></td>
<td><p>This class implements different methods to load / write data from and to a yaml file.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="loaders.configloader.ConfigLoader">
<em class="property">class </em><code class="sig-prename descclassname">loaders.configloader.</code><code class="sig-name descname">ConfigLoader</code><a class="headerlink" href="#loaders.configloader.ConfigLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class implements different methods to load / write data from
and to a yaml file. It is used to serialize / load all the network hyperparameter.</p>
<dl class="method">
<dt id="loaders.configloader.ConfigLoader.from_file">
<code class="sig-name descname">from_file</code><span class="sig-paren">(</span><em class="sig-param">env_path: str = 'yaml/env.yml'</em>, <em class="sig-param">suppress_print=False</em><span class="sig-paren">)</span> &#x2192; loaders.configloader.ConfigLoader<a class="headerlink" href="#loaders.configloader.ConfigLoader.from_file" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="loaders.configloader.ConfigLoader.from_string">
<code class="sig-name descname">from_string</code><span class="sig-paren">(</span><em class="sig-param">data: str</em><span class="sig-paren">)</span> &#x2192; loaders.configloader.ConfigLoader<a class="headerlink" href="#loaders.configloader.ConfigLoader.from_string" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="loaders.configloader.ConfigLoader.switch">
<code class="sig-name descname">switch</code><span class="sig-paren">(</span><em class="sig-param">key: str, options: Dict[str, Callable]</em><span class="sig-paren">)</span><a class="headerlink" href="#loaders.configloader.ConfigLoader.switch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="loaders.configloader.ConfigLoader.to_json">
<code class="sig-name descname">to_json</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#loaders.configloader.ConfigLoader.to_json" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<span class="target" id="module-modules.autoencoder"></span><div class="section" id="autoencoder-py">
<h2>autoencoder.py<a class="headerlink" href="#autoencoder-py" title="Permalink to this headline">¶</a></h2>
<p>First implementation of a horizontal (layerwise) training mechanism of a deep network.
It is fundamentally flawed because the receptive field of this stack does not
increase in depth and therefore it is not possible for the network to learn
complex features in images.</p>
<p>This happens because the upstream maps the data back to the input size of this
autoencoder to make it stackable.</p>
<p>Replaced by SidecarAutoencoder which trains a narrowing network that does not have
these problems. We keep the code for reasoning / documentation of our mistakes.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.Autoencoder</span></code>([color_channels, dropout])</p></td>
<td><p>Implementation of an autoencoder used in our stack.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.SupervisedAutoencoder</span></code>(color_channels)</p></td>
<td><p>A supervised extension to the autoencoder class.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.autoencoder.Autoencoder">
<em class="property">class </em><code class="sig-prename descclassname">modules.autoencoder.</code><code class="sig-name descname">Autoencoder</code><span class="sig-paren">(</span><em class="sig-param">color_channels: int = 3</em>, <em class="sig-param">dropout: int = 0.3</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.autoencoder.Autoencoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference internal" href="#modules.stackable_network.StackableNetwork" title="modules.stackable_network.StackableNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">modules.stackable_network.StackableNetwork</span></code></a></p>
<p>Implementation of an autoencoder used in our stack. This is a horizontal module.
It is connected to the upper layer via its upstream function.</p>
<dl class="method">
<dt id="modules.autoencoder.Autoencoder.calculate_upstream">
<code class="sig-name descname">calculate_upstream</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.autoencoder.Autoencoder.calculate_upstream" title="Permalink to this definition">¶</a></dt>
<dd><p>computes the representation of x used in the next layer</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="modules.autoencoder.SupervisedAutoencoder">
<em class="property">class </em><code class="sig-prename descclassname">modules.autoencoder.</code><code class="sig-name descname">SupervisedAutoencoder</code><span class="sig-paren">(</span><em class="sig-param">color_channels: int</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.autoencoder.SupervisedAutoencoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#modules.autoencoder.Autoencoder" title="modules.autoencoder.Autoencoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">modules.autoencoder.Autoencoder</span></code></a></p>
<p>A supervised extension to the autoencoder class.
It extends autoencoder with a fully connected module linked to the autoencoder
bottleneck.</p>
</dd></dl>

</div>
<span class="target" id="module-modules.fcview"></span><div class="section" id="fcview-py">
<h2>fcview.py<a class="headerlink" href="#fcview-py" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.FCView</span></code>()</p></td>
<td><p>Pytorch view abstraction as nn.module</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.fcview.FCView">
<em class="property">class </em><code class="sig-prename descclassname">modules.fcview.</code><code class="sig-name descname">FCView</code><a class="headerlink" href="#modules.fcview.FCView" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Pytorch view abstraction as nn.module</p>
</dd></dl>

</div>
<span class="target" id="module-modules.inheritance"></span><p>Experimental class not used in the final product</p>
<dl class="class">
<dt id="modules.inheritance.Autoencoder">
<em class="property">class </em><code class="sig-prename descclassname">modules.inheritance.</code><code class="sig-name descname">Autoencoder</code><span class="sig-paren">(</span><em class="sig-param">color_channels=3</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.inheritance.Autoencoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="attribute">
<dt id="modules.inheritance.Autoencoder.decoder">
<code class="sig-name descname">decoder</code><em class="property"> = None</em><a class="headerlink" href="#modules.inheritance.Autoencoder.decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="modules.inheritance.StackableNetwork">
<em class="property">class </em><code class="sig-prename descclassname">modules.inheritance.</code><code class="sig-name descname">StackableNetwork</code><a class="headerlink" href="#modules.inheritance.StackableNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="modules.inheritance.StackableNetwork.abstract_method">
<code class="sig-name descname">abstract_method</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#modules.inheritance.StackableNetwork.abstract_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="modules.inheritance.SupervisedAutoencoder">
<em class="property">class </em><code class="sig-prename descclassname">modules.inheritance.</code><code class="sig-name descname">SupervisedAutoencoder</code><span class="sig-paren">(</span><em class="sig-param">color_channels</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.inheritance.SupervisedAutoencoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#modules.inheritance.Autoencoder" title="modules.inheritance.Autoencoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">modules.inheritance.Autoencoder</span></code></a>, <a class="reference internal" href="#modules.inheritance.StackableNetwork" title="modules.inheritance.StackableNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">modules.inheritance.StackableNetwork</span></code></a></p>
<dl class="method">
<dt id="modules.inheritance.SupervisedAutoencoder.abstract_method">
<code class="sig-name descname">abstract_method</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#modules.inheritance.SupervisedAutoencoder.abstract_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="modules.inheritance.SupervisedAutoencoder.supervision">
<code class="sig-name descname">supervision</code><em class="property"> = None</em><a class="headerlink" href="#modules.inheritance.SupervisedAutoencoder.supervision" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="modules.inheritance.SupervisedAutoencoder.test">
<code class="sig-name descname">test</code><em class="property"> = 1</em><a class="headerlink" href="#modules.inheritance.SupervisedAutoencoder.test" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<span class="target" id="module-modules.interpolate"></span><div class="section" id="interpolate-py">
<h2>interpolate.py<a class="headerlink" href="#interpolate-py" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.Interpolate</span></code>([scale_factor, mode])</p></td>
<td><p>Interpolate function used for inverse convolution in Decoders</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.interpolate.Interpolate">
<em class="property">class </em><code class="sig-prename descclassname">modules.interpolate.</code><code class="sig-name descname">Interpolate</code><span class="sig-paren">(</span><em class="sig-param">scale_factor=2</em>, <em class="sig-param">mode='nearest'</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.interpolate.Interpolate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Interpolate function used for inverse convolution in
Decoders</p>
</dd></dl>

</div>
<span class="target" id="module-modules.maps"></span><div class="section" id="maps-py">
<h2>maps.py<a class="headerlink" href="#maps-py" title="Permalink to this headline">¶</a></h2>
<p>The term map in this project is a nn.module that maps data from one horizontal
layer to the next. It is the abstract concept of coupling such individually 
trained layers into one bigger (deeper) network.</p>
<p>Many different maps are collected in this module. It one can check in the config
yaml to see which of these maps is used in a specific training instance.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.RandomMap</span></code>(in_shape, out_shape)</p></td>
<td><p>This map randomly permutes the input and maps it to the output shape.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.SidecarMap</span></code>(main_network_layer)</p></td>
<td><p>This map takes any set of non-trainable layer from a bigger main network and uses it as a map function.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.ConvMap</span></code>(in_shape, out_shape)</p></td>
<td><p>Trainable map that is a one layer convolution to map from input to output dimension.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.DecoderMap</span></code>(trained_net)</p></td>
<td><p>This map takes the last layer of a horizontal autoencoder and uses it as an upstream mapping initialization.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.maps.ConvMap">
<em class="property">class </em><code class="sig-prename descclassname">modules.maps.</code><code class="sig-name descname">ConvMap</code><span class="sig-paren">(</span><em class="sig-param">in_shape: Tuple[int, int, int], out_shape: Tuple[int, int, int]</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.maps.ConvMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Trainable map that is a one layer convolution
to map from input to output dimension.</p>
</dd></dl>

<dl class="class">
<dt id="modules.maps.DecoderMap">
<em class="property">class </em><code class="sig-prename descclassname">modules.maps.</code><code class="sig-name descname">DecoderMap</code><span class="sig-paren">(</span><em class="sig-param">trained_net: modules.autoencoder.Autoencoder</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.maps.DecoderMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>This map takes the last layer of a horizontal autoencoder and
uses it as an upstream mapping initialization.
No scaling is needed since this layer has
the exact dimensions we need by design.</p>
<p>Other than that, it is similar to ConvMap.</p>
</dd></dl>

<dl class="class">
<dt id="modules.maps.RandomMap">
<em class="property">class </em><code class="sig-prename descclassname">modules.maps.</code><code class="sig-name descname">RandomMap</code><span class="sig-paren">(</span><em class="sig-param">in_shape: Tuple[int, int, int], out_shape: Tuple[int, int, int]</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.maps.RandomMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>This map randomly permutes the input and maps it
to the output shape.</p>
</dd></dl>

<dl class="class">
<dt id="modules.maps.SidecarMap">
<em class="property">class </em><code class="sig-prename descclassname">modules.maps.</code><code class="sig-name descname">SidecarMap</code><span class="sig-paren">(</span><em class="sig-param">main_network_layer: List[torch.nn.modules.module.Module]</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.maps.SidecarMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>This map takes any set of
non-trainable layer from a bigger main network
and uses it as a map function.</p>
<p>Mainly used for maxpool layers.</p>
</dd></dl>

</div>
<span class="target" id="module-modules.network_stack"></span><div class="section" id="network-stack-py">
<h2>network_stack.py<a class="headerlink" href="#network-stack-py" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.NetworkStack</span></code>(networks[, …])</p></td>
<td><p>A network stack is responsible to use a set of layers and maps to:</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.network_stack.NetworkStack">
<em class="property">class </em><code class="sig-prename descclassname">modules.network_stack.</code><code class="sig-name descname">NetworkStack</code><span class="sig-paren">(</span><em class="sig-param">networks: List[Tuple[modules.stackable_network.StackableNetwork, torch.nn.modules.module.Module]], train_every_pass=False</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.network_stack.NetworkStack" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A network stack is responsible to use a set of
layers and maps to:</p>
<ul class="simple">
<li><p>Compute the upstream to this layer</p></li>
<li><p>Train that one layer horizontally</p></li>
</ul>
<p>The class is initialized with a reference to all the
previous layers</p>
<dl class="method">
<dt id="modules.network_stack.NetworkStack.upwards">
<code class="sig-name descname">upwards</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.network_stack.NetworkStack.upwards" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<span class="target" id="module-modules.stackable_network"></span><div class="section" id="stackable-network-py">
<h2>stackable_network.py<a class="headerlink" href="#stackable-network-py" title="Permalink to this headline">¶</a></h2>
<p>Interface class that is used in classes that implement the
specification necessary to be used as a horizontal training layer.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.StackableNetwork</span></code>()</p></td>
<td><p>Interface, not a Class! Do not implement anything here Classes that inherit from StackableNetwork are required, (in python by convention, not enforced) to provide an implementation of these functions.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.stackable_network.StackableNetwork">
<em class="property">class </em><code class="sig-prename descclassname">modules.stackable_network.</code><code class="sig-name descname">StackableNetwork</code><a class="headerlink" href="#modules.stackable_network.StackableNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Interface, not a Class! Do not implement anything here
Classes that inherit from StackableNetwork are required,
(in python by convention, not enforced) to provide an
implementation of these functions.</p>
<dl class="method">
<dt id="modules.stackable_network.StackableNetwork.calculate_upstream">
<code class="sig-name descname">calculate_upstream</code><span class="sig-paren">(</span><em class="sig-param">previous_network</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.stackable_network.StackableNetwork.calculate_upstream" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the output to the point where the
map function will be applied after.</p>
<p>Make sure those layers appear in parameters()</p>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-modules.vgg_autoencoder"></span><div class="section" id="vgg-autoencoder-py">
<h2>vgg_autoencoder.py<a class="headerlink" href="#vgg-autoencoder-py" title="Permalink to this headline">¶</a></h2>
<p>This module contains all autoencoders required to train a
VGG network horizontally.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.SidecarAutoencoder</span></code>(…)</p></td>
<td><p>This autoencoder is a “sidecar” to a deeper network.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.SupervisedSidecarAutoencoder</span></code>(…)</p></td>
<td><p>The supervision extension of the sidecar autoencoder.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.vgg_autoencoder.SidecarAutoencoder">
<em class="property">class </em><code class="sig-prename descclassname">modules.vgg_autoencoder.</code><code class="sig-name descname">SidecarAutoencoder</code><span class="sig-paren">(</span><em class="sig-param">main_network_layer: List[torch.nn.modules.module.Module], img_size: int, channels: Tuple[int, int], dropout: float, kernel_size: int, encoder_type: str</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.vgg_autoencoder.SidecarAutoencoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>This autoencoder is a “sidecar” to a deeper network. It takes a slice of
the main network (main_network_layer) and trains this layer isolated from the
other parts of the main network.</p>
<p>Different autoencoder types are supported (A-C).</p>
<dl class="method">
<dt id="modules.vgg_autoencoder.SidecarAutoencoder.bottleneck_size">
<code class="sig-name descname">bottleneck_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#modules.vgg_autoencoder.SidecarAutoencoder.bottleneck_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="modules.vgg_autoencoder.SidecarAutoencoder.calculate_upstream">
<code class="sig-name descname">calculate_upstream</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.vgg_autoencoder.SidecarAutoencoder.calculate_upstream" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="modules.vgg_autoencoder.SupervisedSidecarAutoencoder">
<em class="property">class </em><code class="sig-prename descclassname">modules.vgg_autoencoder.</code><code class="sig-name descname">SupervisedSidecarAutoencoder</code><span class="sig-paren">(</span><em class="sig-param">main_network_layer: List[torch.nn.modules.module.Module], img_size: int, channels: Tuple[int, int], dropout: float, kernel_size: int, encoder_type: int, num_classes: int</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.vgg_autoencoder.SupervisedSidecarAutoencoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#modules.vgg_autoencoder.SidecarAutoencoder" title="modules.vgg_autoencoder.SidecarAutoencoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">modules.vgg_autoencoder.SidecarAutoencoder</span></code></a></p>
<p>The supervision extension of the sidecar autoencoder.</p>
</dd></dl>

</div>
<span class="target" id="module-modules.vgg"></span><div class="section" id="vgg-py">
<h2>vgg.py<a class="headerlink" href="#vgg-py" title="Permalink to this headline">¶</a></h2>
<p>This is the main vertical network of this project. We use / modified the VGG
implementation from <a class="reference external" href="https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py">https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py</a></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules.VGG</span></code>(num_classes, dropout, img_size, …)</p></td>
<td><p>VGG CNN Implementation with minor changes such that we can extract all layers to train them individually.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="modules.vgg.VGG">
<em class="property">class </em><code class="sig-prename descclassname">modules.vgg.</code><code class="sig-name descname">VGG</code><span class="sig-paren">(</span><em class="sig-param">num_classes: int</em>, <em class="sig-param">dropout: float</em>, <em class="sig-param">img_size: int</em>, <em class="sig-param">vgg_version: str</em>, <em class="sig-param">batch_norm: bool</em><span class="sig-paren">)</span><a class="headerlink" href="#modules.vgg.VGG" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>VGG CNN Implementation with minor changes such that
we can extract all layers to train them individually.</p>
<p>Layers are trained via sidecar autoencoders.
The pooling layers become upstream maps.</p>
<dl class="method">
<dt id="modules.vgg.VGG.get_trainable_modules">
<code class="sig-name descname">get_trainable_modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; List[Tuple[List[torch.nn.modules.module.Module], Tuple[int, int], int, List[torch.nn.modules.module.Module]]]<a class="headerlink" href="#modules.vgg.VGG.get_trainable_modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="modules.vgg.VGG.make_modules">
<code class="sig-name descname">make_modules</code><span class="sig-paren">(</span><em class="sig-param">cfg</em>, <em class="sig-param">img_size</em>, <em class="sig-param">batch_norm: bool</em><span class="sig-paren">)</span> &#x2192; Tuple[List[torch.nn.modules.module.Module], List[Tuple[List[torch.nn.modules.module.Module], Tuple[int, int], int, List[torch.nn.modules.module.Module]]]]<a class="headerlink" href="#modules.vgg.VGG.make_modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.encodernet"></span><div class="section" id="encodernet-py">
<h2>encodernet.py<a class="headerlink" href="#encodernet-py" title="Permalink to this headline">¶</a></h2>
<p>This module combines all the custom network modules and defines
the schedule how to train and evaluate our networks.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#networks.encodernet.AutoencoderNet" title="networks.encodernet.AutoencoderNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">networks.encodernet.AutoencoderNet</span></code></a>(gcfg, rcfg)</p></td>
<td><p>Main Class of this project.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="networks.encodernet.AutoencoderNet">
<em class="property">class </em><code class="sig-prename descclassname">networks.encodernet.</code><code class="sig-name descname">AutoencoderNet</code><span class="sig-paren">(</span><em class="sig-param">gcfg: loaders.configloader.ConfigLoader</em>, <em class="sig-param">rcfg: loaders.configloader.ConfigLoader</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Main Class of this project. Groups all the functions to run a network 
but is not responsible to assemble the network configuration /  hyperparameters. 
Most of the functions accept a LayerTrainingDefinition 
or a ConfigLoader to access the training parameters.</p>
<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.majority_vote">
<code class="sig-name descname">majority_vote</code><span class="sig-paren">(</span><em class="sig-param">configs: List[networks.layer_training_def.LayerTrainingDefinition], global_epoch: int</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.majority_vote" title="Permalink to this definition">¶</a></dt>
<dd><p>This function implements a voting scheme over all layers.
It uses a layer-wise softmax prediction. The final decision is taken as argmax
over the mean of the individual distributions.</p>
</dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.measure_time">
<code class="sig-name descname">measure_time</code><span class="sig-paren">(</span><em class="sig-param">t_start</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.measure_time" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.plot_img">
<code class="sig-name descname">plot_img</code><span class="sig-paren">(</span><em class="sig-param">real_imgs</em>, <em class="sig-param">dc_imgs</em>, <em class="sig-param">epoch</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.plot_img" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.test">
<code class="sig-name descname">test</code><span class="sig-paren">(</span><em class="sig-param">epoch: int</em>, <em class="sig-param">global_epoch: int</em>, <em class="sig-param">config: networks.layer_training_def.LayerTrainingDefinition</em>, <em class="sig-param">plot_every_n_epochs=1</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.test" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is the default test method for a horizontal autoencoder network block.
It operates on a given config of type LayerTrainingDefinition.
The method freezes the current layer and computes the test accuracy.</p>
</dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.test_vgg_classifier">
<code class="sig-name descname">test_vgg_classifier</code><span class="sig-paren">(</span><em class="sig-param">epoch: int</em>, <em class="sig-param">global_epoch: int</em>, <em class="sig-param">config: networks.layer_training_def.LayerTrainingDefinition</em>, <em class="sig-param">plot_every_n_epochs=1</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.test_vgg_classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>The vgg network has its own classification layer with a cost function that differs from the usual
autoencoder network block. Therefore it has its own test method.</p>
</dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.to_img">
<code class="sig-name descname">to_img</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.to_img" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">epoch: int</em>, <em class="sig-param">global_epoch: int</em>, <em class="sig-param">config: networks.layer_training_def.LayerTrainingDefinition</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.train" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is the default train method for a horizontal autoencoder network block.
It takes care of dataset iteration and loss calculation / optimization.
It operates on a given config of type LayerTrainingDefinition.
This code represents what needs to be done in one epoch of training.</p>
</dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.train_test">
<code class="sig-name descname">train_test</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.train_test" title="Permalink to this definition">¶</a></dt>
<dd><p>This function defines the overall training schedule over the total number of epochs.
It trains the layer one by one:</p>
<ul class="simple">
<li><p>First layer 0 is trained for n epochs</p></li>
<li><p>Then layer 1 is trained for n epochs</p></li>
<li><p>etc…</p></li>
</ul>
<p>It is called from the main script after class initialization to start the training.</p>
</dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.train_vgg_classifier">
<code class="sig-name descname">train_vgg_classifier</code><span class="sig-paren">(</span><em class="sig-param">epoch: int</em>, <em class="sig-param">global_epoch: int</em>, <em class="sig-param">config: networks.layer_training_def.LayerTrainingDefinition</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.train_vgg_classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>The vgg network has its own classification layer with a cost function that differs from the usual
autoencoder network block. Therefore it has its own training method.</p>
</dd></dl>

<dl class="method">
<dt id="networks.encodernet.AutoencoderNet.wave_train_test">
<code class="sig-name descname">wave_train_test</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.AutoencoderNet.wave_train_test" title="Permalink to this definition">¶</a></dt>
<dd><p>This function defines the overall training schedule over the total number of epochs.
It trains the layer in a wave-like pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>Layer 0 is trained for (total_epochs / waves) epochs</p></li>
<li><p>Layer 1 is trained for (total_epochs / waves) epochs</p></li>
</ul>
</div></blockquote>
<p>after the final layer is trained, we return back to layer 0.</p>
<p>This method converges much quicker than strict sequential training.</p>
<p>It is called from the main script after class initialization to start the training.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="networks.encodernet.ensure_dir">
<code class="sig-prename descclassname">networks.encodernet.</code><code class="sig-name descname">ensure_dir</code><span class="sig-paren">(</span><em class="sig-param">path: str</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.ensure_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="networks.encodernet.save_layer">
<code class="sig-prename descclassname">networks.encodernet.</code><code class="sig-name descname">save_layer</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">path: str</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.encodernet.save_layer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<span class="target" id="module-networks.cfg_to_network"></span><div class="section" id="cfg-to-network-py">
<h2>cfg_to_network.py<a class="headerlink" href="#cfg-to-network-py" title="Permalink to this headline">¶</a></h2>
<p>Module to map from configuration files to classes of our network.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#module-networks.cfg_to_network" title="networks.cfg_to_network"><code class="xref py py-obj docutils literal notranslate"><span class="pre">networks.cfg_to_network</span></code></a>(gcfg, rcfg)</p></td>
<td><p>This is the main network assembly function.</p></td>
</tr>
</tbody>
</table>
<dl class="function">
<dt id="networks.cfg_to_network.cfg_to_network">
<code class="sig-prename descclassname">networks.cfg_to_network.</code><code class="sig-name descname">cfg_to_network</code><span class="sig-paren">(</span><em class="sig-param">gcfg: loaders.configloader.ConfigLoader</em>, <em class="sig-param">rcfg: loaders.configloader.ConfigLoader</em><span class="sig-paren">)</span> &#x2192; List[networks.layer_training_def.LayerTrainingDefinition]<a class="headerlink" href="#networks.cfg_to_network.cfg_to_network" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the main network assembly function. It takes a config 
from a configloader (a yaml file). Assembles a network stack
as list of LayerTrainingDefintions.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>List[LayerTrainingDefinition]</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="networks.cfg_to_network.load_layer">
<code class="sig-prename descclassname">networks.cfg_to_network.</code><code class="sig-name descname">load_layer</code><span class="sig-paren">(</span><em class="sig-param">layer: torch.nn.modules.module.Module</em>, <em class="sig-param">path: str</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.cfg_to_network.load_layer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="networks.cfg_to_network.vgg_sidecar_layer">
<code class="sig-prename descclassname">networks.cfg_to_network.</code><code class="sig-name descname">vgg_sidecar_layer</code><span class="sig-paren">(</span><em class="sig-param">vgg: modules.vgg.VGG</em>, <em class="sig-param">index: int</em>, <em class="sig-param">dropout: float</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">encoder_type: int</em>, <em class="sig-param">num_classes: int</em><span class="sig-paren">)</span> &#x2192; torch.nn.modules.module.Module<a class="headerlink" href="#networks.cfg_to_network.vgg_sidecar_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>helper function used in cfg_to_network to assemble a
SupervisedSidecarAutoencoder</p>
</dd></dl>

</div>
<span class="target" id="module-networks.layer_training_def"></span><div class="section" id="layer-training-def-py">
<h2>layer_training_def.py<a class="headerlink" href="#layer-training-def-py" title="Permalink to this headline">¶</a></h2>
<p>Wrappers / Definitions needed in our network.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">networks.LayerTrainingDefinition</span></code>([…])</p></td>
<td><p>This class is a wrapper for all objects that are needed in a layer training process.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#networks.layer_training_def.LayerType" title="networks.layer_training_def.LayerType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerType</span></code></a></p></td>
<td><p><ul class="simple">
<li><p>VGGlinear : the last layer of our VGG network</p></li>
</ul>
</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="networks.layer_training_def.LayerTrainingDefinition">
<em class="property">class </em><code class="sig-prename descclassname">networks.layer_training_def.</code><code class="sig-name descname">LayerTrainingDefinition</code><span class="sig-paren">(</span><em class="sig-param">layer_type: networks.layer_training_def.LayerType = None</em>, <em class="sig-param">layer_name: str = None</em>, <em class="sig-param">num_epochs: int = 0</em>, <em class="sig-param">pretraining_store: str = None</em>, <em class="sig-param">pretraining_load: str = None</em>, <em class="sig-param">model_base_path: str = None</em>, <em class="sig-param">stack: modules.network_stack.NetworkStack = None</em>, <em class="sig-param">upstream: torch.nn.modules.module.Module = None</em>, <em class="sig-param">model: torch.nn.modules.module.Module = None</em>, <em class="sig-param">tp_alpha: torch.nn.parameter.Parameter = None</em>, <em class="sig-param">optimizer: torch.optim.optimizer.Optimizer = None</em>, <em class="sig-param">ae_loss_function: str = None</em><span class="sig-paren">)</span><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class is a wrapper for all objects that
are needed in a layer training process.</p>
<p>The property stack contains a reference to all
the previous layers such that the upstream
(representation of x before this layer)
can be calculated.</p>
<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.ae_loss_function">
<code class="sig-name descname">ae_loss_function</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.ae_loss_function" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.layer_name">
<code class="sig-name descname">layer_name</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.layer_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.layer_type">
<code class="sig-name descname">layer_type</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.layer_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.model">
<code class="sig-name descname">model</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.model_base_path">
<code class="sig-name descname">model_base_path</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.model_base_path" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.num_epochs">
<code class="sig-name descname">num_epochs</code><em class="property"> = 0</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.num_epochs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.optimizer">
<code class="sig-name descname">optimizer</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.optimizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.pretraining_load">
<code class="sig-name descname">pretraining_load</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.pretraining_load" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.pretraining_store">
<code class="sig-name descname">pretraining_store</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.pretraining_store" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.stack">
<code class="sig-name descname">stack</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.stack" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.tp_alpha">
<code class="sig-name descname">tp_alpha</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.tp_alpha" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerTrainingDefinition.upstream">
<code class="sig-name descname">upstream</code><em class="property"> = None</em><a class="headerlink" href="#networks.layer_training_def.LayerTrainingDefinition.upstream" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="networks.layer_training_def.LayerType">
<em class="property">class </em><code class="sig-prename descclassname">networks.layer_training_def.</code><code class="sig-name descname">LayerType</code><a class="headerlink" href="#networks.layer_training_def.LayerType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<ul class="simple">
<li><p>VGGlinear : the last layer of our VGG network</p></li>
<li><p>Stack: any other layer trained by a supervised autoencoder</p></li>
</ul>
<dl class="attribute">
<dt id="networks.layer_training_def.LayerType.Stack">
<code class="sig-name descname">Stack</code><em class="property"> = 1</em><a class="headerlink" href="#networks.layer_training_def.LayerType.Stack" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="networks.layer_training_def.LayerType.VGGlinear">
<code class="sig-name descname">VGGlinear</code><em class="property"> = 0</em><a class="headerlink" href="#networks.layer_training_def.LayerType.VGGlinear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">scaling-spoon</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Kaan Oktay, Jonas Frey, Raffael Theiler.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.3.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>