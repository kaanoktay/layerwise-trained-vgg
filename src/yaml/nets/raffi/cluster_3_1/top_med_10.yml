weight_decay: 0.0 # ADAM decay
color_channels: 3
supervised_ratio: 0.1
augmentation: True
batch_size: 128
test_every_n_epochs: 1

pred_loss_weight: 0.7 # "trainable" or  number (0,1)"

# us*(1-a)+(s+p)*a+r
# (us+s)*(1-a)+p*a+r
# (us+10*s)*(1-a)+p*a+r
ae_loss_function: (us+s)*(1-a)+p*a+r

vgg_version: A
vgg_dropout: 0.5

# Persistence path for later analysis
model_path: trained_models/top_med_10

# must be configured in env.yml 
dataset: cifar10 # Supports mnist|cifar10
dataset_transform: med_10

decoding_criterion: MSELoss # Supports: MSELoss
prediction_criterion: CrossEntropyLoss # Supports: CrossEntropyLoss

# pretraining_store must be a path !
# pretraining_load must be a file name !
layers:

  # Layer 0 (64)
  - num_epoch: 50
    encoder_type: A
    learning_rate: 0.001
    kernel_size: 3
    dropout_rate: 0.7
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load: null
    pretraining_store: True
    upstream_params: null

  # Layer 1 (64)
  - num_epoch: 50
    kernel_size: 3
    encoder_type: A
    learning_rate: 0.001
    dropout_rate: 0.6
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: True
    upstream_params: null

  # Layer 2 (64)
  - num_epoch: 50
    kernel_size: 3
    encoder_type: B
    learning_rate: 0.001
    dropout_rate: 0.5
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: True
    upstream_params: null

  # Layer 3 (128)
  - num_epoch: 50
    kernel_size: 3
    encoder_type: B
    learning_rate: 0.001
    dropout_rate: 0.5
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: True
    upstream_params: null

  # Layer 4 (128)
  - num_epoch: 50
    kernel_size: 3
    encoder_type: C
    learning_rate: 0.001
    dropout_rate: 0.5
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: True
    upstream_params: null

  # Layer 5 (256)
  - num_epoch: 50
    kernel_size: 3
    encoder_type: C
    learning_rate: 0.001
    dropout_rate: 0.6
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: True
    upstream_params: null

  # Layer 6 (256)
  - num_epoch: 50
    kernel_size: 3
    encoder_type: C
    learning_rate: 0.001
    dropout_rate: 0.6
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: True
    upstream_params: null

  # Layer 7 (256)
  - num_epoch: 50
    kernel_size: 3
    encoder_type: C
    learning_rate: 0.001
    dropout_rate: 0.6
    model: VGGn # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: True
    upstream_params: null


    # VGG Linear Layer (256)
  - num_epoch: 50
    dropout_rate: 0.5
    learning_rate: 0.001
    model: VGGlinear # VGG n-th layer
    optimizer: Adam
    pretraining_load:  null
    pretraining_store: False
    upstream_params: null
